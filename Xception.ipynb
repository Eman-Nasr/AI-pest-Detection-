{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eman-Nasr/AI-pest-Detection-/blob/main/Xception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRNyfClHDdnW",
        "outputId": "ed998de2-e6b1-421a-a3f3-e082a1a55ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/My Drive/insect\"\n"
      ],
      "metadata": {
        "id": "iwIUtbsfDt6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(\"/content/drive/My Drive/insect\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrPuNPC_D6xP",
        "outputId": "e065fac9-20ef-4fb0-a31d-37809c5457dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inWxt5w6_N71",
        "outputId": "d67fb66f-2fe7-40a7-a4a0-2a6ed8a147e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6772 files belonging to 2 classes.\n",
            "Using 5418 files for training.\n",
            "Found 6772 files belonging to 2 classes.\n",
            "Using 1354 files for validation.\n",
            "Classes: ['harmful', 'not_harm']\n",
            "History file not found. Training will start from scratch.\n",
            "Loaded checkpoint model from: /content/drive/My Drive/xception_checkpoint.keras\n",
            "Epoch 1/3\n",
            "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5840s\u001b[0m 34s/step - accuracy: 0.9278 - loss: 0.1969 - val_accuracy: 0.4963 - val_loss: 1.9849 - learning_rate: 3.0000e-04\n",
            "Epoch 2/3\n",
            "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5700s\u001b[0m 33s/step - accuracy: 0.9815 - loss: 0.0625 - val_accuracy: 0.7740 - val_loss: 1.6950 - learning_rate: 3.0000e-04\n",
            "Epoch 3/3\n",
            "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5730s\u001b[0m 33s/step - accuracy: 0.9904 - loss: 0.0270 - val_accuracy: 0.6928 - val_loss: 1.3905 - learning_rate: 3.0000e-04\n",
            "Model training continued and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "\n",
        "# Define optimized image parameters\n",
        "img_height, img_width = 224, 224  # Higher resolution for better accuracy\n",
        "batch_size = 32  # Balanced batch size for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Define paths for saving model progress\n",
        "checkpoint_xception = \"/content/drive/My Drive/xception_checkpoint.keras\"\n",
        "history_file = \"/content/drive/My Drive/xception_history.txt\"\n",
        "final_model_path = \"/content/drive/My Drive/insect_classifier_xception.keras\"\n",
        "\n",
        "# Load dataset\n",
        "new_train_ds_raw = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/My Drive/insect\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "new_val_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/My Drive/insect\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Get class names BEFORE applying `.map()`\n",
        "class_names = new_train_ds_raw.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# Apply Advanced Data Augmentation\n",
        "new_data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.3),\n",
        "    layers.RandomZoom(0.3),\n",
        "    layers.RandomContrast(0.2),\n",
        "    layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
        "])\n",
        "\n",
        "# Apply `.map()` AFTER getting class names\n",
        "new_train_ds = new_train_ds_raw.map(lambda x, y: (new_data_augmentation(x, training=True), y))\n",
        "new_train_ds = new_train_ds.cache().prefetch(buffer_size=AUTOTUNE)  # Speed up training\n",
        "new_val_ds = new_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Load Pre-trained Xception Model (Higher Accuracy than MobileNetV2)\n",
        "xception_base = keras.applications.Xception(input_shape=(img_height, img_width, 3),\n",
        "                                            include_top=False,\n",
        "                                            weights='imagenet')\n",
        "\n",
        "# Unfreeze last 30 layers for fine-tuning\n",
        "for layer in xception_base.layers[-30:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Build the Fine-Tuned Model\n",
        "xception_model = keras.Sequential([\n",
        "    xception_base,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.2),  # Reduced dropout for better learning\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Use an optimized learning rate with decay\n",
        "xception_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "# Save Training Progress (Checkpoint)\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_xception,\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor='val_accuracy',\n",
        "                                                      mode='max')\n",
        "\n",
        "# Learning Rate Adjustment\n",
        "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
        "\n",
        "# Define the number of additional epochs to train\n",
        "additional_epochs = 3\n",
        "last_epoch = 0  # Default to starting from scratch\n",
        "\n",
        "# **Step 1: Check for a saved history file**\n",
        "if os.path.exists(history_file):\n",
        "    with open(history_file, \"r\") as f:\n",
        "        content = f.read().strip()  # Read and strip whitespace\n",
        "        if content.isdigit():\n",
        "            last_epoch = int(content)  # Convert only if valid number\n",
        "            print(f\"Resuming training from epoch {last_epoch}...\")\n",
        "        else:\n",
        "            print(\"History file is empty. Starting training from scratch.\")\n",
        "            last_epoch = 0  # Start from scratch\n",
        "else:\n",
        "    print(\"History file not found. Training will start from scratch.\")\n",
        "\n",
        "# **Step 2: Load model checkpoint if available**\n",
        "if os.path.exists(checkpoint_xception):\n",
        "    xception_model = keras.models.load_model(checkpoint_xception)\n",
        "    print(\"Loaded checkpoint model from:\", checkpoint_xception)\n",
        "\n",
        "# **Step 3: Train the model from the last saved epoch**\n",
        "history_xception = xception_model.fit(new_train_ds, validation_data=new_val_ds,\n",
        "                                      epochs=last_epoch + additional_epochs,\n",
        "                                      initial_epoch=last_epoch,\n",
        "                                      callbacks=[checkpoint_callback, lr_scheduler])\n",
        "\n",
        "# **Step 4: Save the last trained epoch number**\n",
        "with open(history_file, \"w\") as f:\n",
        "    f.write(str(last_epoch + additional_epochs))\n",
        "\n",
        "# **Step 5: Save final model after training**\n",
        "xception_model.save(final_model_path)\n",
        "print(\"Model training continued and saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5jFPq_ylsgR",
        "outputId": "258fdc05-6931-4b1e-d816-fff688bff3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'\t\t     mobilenet_checkpoint.keras\n",
            " Data\t\t\t\t     mobilenet_training_history.json\n",
            " harmful_insect_classifier.keras     not_harm_insect_classifier.keras\n",
            " insect\t\t\t\t     risk_classifier.keras\n",
            " insect_classifier.keras\t     xception_checkpoint.keras\n",
            " insect_classifier_mobilenet.keras   xception_history.json\n",
            " insect_classifier_xception.keras    xception_history.txt\n",
            " insect_training_history.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uRouUWzetzJ",
        "outputId": "bd90a2e8-b82c-4718-a823-c86e7a041443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plots training and validation accuracy and loss.\n",
        "\n",
        "    Parameters:\n",
        "        history: Keras history object returned from model.fit()\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(epochs, history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
        "    plt.plot(epochs, history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(epochs, history.history[\"loss\"], label=\"Training Loss\")\n",
        "    plt.plot(epochs, history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oOIAAgxEfNuC"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}